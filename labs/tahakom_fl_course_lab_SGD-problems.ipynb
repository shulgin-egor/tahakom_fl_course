{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCCYx8II1HxK"
   },
   "source": [
    "# Stochastic Gradient Descent Lab Exercises\n",
    "\n",
    "In this laboratory session, we will investigate the various characteristics associated with Stochastic Gradient Descent, as elucidated in the instructional lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obKDRdtKszZd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import os\n",
    "from numpy.linalg import norm\n",
    "from IPython import display\n",
    "import matplotlib.ticker as tck\n",
    "from mpl_toolkits.mplot3d import axes3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x, y) = \\frac12 (f_1(x_1, x_2) + f_2(x_1, x_2)),\n",
    "$$\n",
    "where \n",
    "$$\n",
    "f_1(x_1, x_2) = a (x_1-c)^2 + b (x_2-d)^2, \\ \\ f_2(x_1, x_2) = a (x_1+c)^2 + b (x_2+d)^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.5\n",
    "b = 1\n",
    "c = 1\n",
    "d = 2\n",
    "\n",
    "f = lambda x1, x2: a*x1**2 + b*x2**2 + c ** 2 + d ** 2\n",
    "grad_f = lambda x1, x2: np.array([2*a*x1, 2*b*x2])\n",
    "hess_f = [2*a, 2*b] #in this case hessian is the diagonal matrix\n",
    "\n",
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "xmin, xmax = -6,6\n",
    "\n",
    "zmin, zmax = 0,40\n",
    "grid = np.linspace(xmin, xmax, 100)\n",
    "X,Y = np.meshgrid(grid, grid)\n",
    "Z = np.array([[f(x1,x2)  for x1 in grid]  for x2 in grid] )\n",
    "\n",
    "# Plot the 3D surface\n",
    "ax.plot_surface(X, Y, Z, rstride=8, cstride=8, alpha=0.3)\n",
    "\n",
    "#ax.plot_wireframe(X, Y, z_ar[i], label=label_ar[i], color=color_ar[i])\n",
    "\n",
    "ax.contourf(X, Y, Z, zdir='z', offset=zmin, cmap=\"viridis\")\n",
    "ax.contourf(X, Y, Z, zdir='x', offset=xmin, cmap=\"viridis\")\n",
    "ax.contourf(X, Y, Z, zdir='y', offset=zmax, cmap=\"viridis\")\n",
    "size = 15\n",
    "plt.rcParams['xtick.labelsize'] = size\n",
    "plt.rcParams['ytick.labelsize'] = size\n",
    "plt.rcParams['legend.fontsize'] = size\n",
    "plt.rcParams['axes.titlesize'] = size\n",
    "plt.rcParams['axes.labelsize'] = size\n",
    "plt.rcParams[\"figure.figsize\"] = [10,10]\n",
    "ax.set(xlim=(xmin, xmax), ylim=(xmin, xmax), zlim=(0, 50),\n",
    "       xlabel=r'$x_1$', ylabel=r'$x_2$', zlabel=r'$f(x_1,x_2)$')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(func, func_grad, stochastic_grad, random_generator, w0, stepsize, Nsteps=100):\n",
    "    \"\"\" Compute stochastic gradient descent iteration statistics for small dimensional problem \"\"\"\n",
    "    w_ar = np.zeros ((Nsteps, 2),dtype=float)\n",
    "    df_ar = np.zeros ((Nsteps, 2), dtype=float)\n",
    "    func_ar = np.zeros (Nsteps, dtype=float)\n",
    "    w_ar[0,:] = w0.copy()\n",
    "    func_ar[0] = func(w0[0],w0[1])\n",
    "    df_ar[0,:] = func_grad(w0[0], w0[1]).copy()\n",
    "\n",
    "    w = w0.copy()\n",
    "    stoch_indices = random_generator.integers(low=0, high=1, endpoint=True, size=Nsteps)\n",
    "    for i in range(1, Nsteps):\n",
    "        w = w - stepsize * stochastic_grad(w[0], w[1], stoch_indices[i])\n",
    "        w_ar[i,:] = w.copy()\n",
    "        df_ar[i,:] = func_grad(w[0], w[1])\n",
    "        func_ar[i] = func(w[0], w[1])\n",
    "\n",
    "    return w_ar, func_ar, df_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_plot(nsteps):\n",
    "    rng = np.random.default_rng()\n",
    "    \n",
    "    a = 0.5\n",
    "    b = 1\n",
    "    c = 1\n",
    "    d = 2\n",
    "\n",
    "    f = lambda x1, x2: a*x1**2 + b*x2**2 + c ** 2 + d ** 2\n",
    "    grad_f = lambda x1, x2: np.array([2*a*x1, 2*b*x2])\n",
    "    stoch_grad_f = lambda x1, x2, ind: \\\n",
    "        np.array([2 * a * (x1 - c), 2 * b * (x2 - d)]) if ind == 0 else np.array([2 * a * (x1 + c), 2 * b * (x2 + d)])\n",
    "\n",
    "    L = max(2*a,2*b)\n",
    "\n",
    "    constant_step_size = 0.5 / L\n",
    "\n",
    "    w1_0 = 5\n",
    "    w2_0 = 5\n",
    "\n",
    "    x1_min = x2_min = -6\n",
    "    x1_max = x2_max = 6\n",
    "    x1_ar = np.linspace(x1_min, x1_max, 100)\n",
    "    x2_ar = np.linspace(x2_min, x2_max, 100)\n",
    "    f_ar = np.array([ np.array([f(w1_, w2_) for w1_ in x1_ar]) for w2_ in x2_ar])\n",
    "    grad_f_ar = np.array( [ np.array([np.linalg.norm(grad_f(w1_, w2_)) for w1_ in x1_ar]) for w2_ in x2_ar])\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(20,7))\n",
    "\n",
    "    #plt.rcParams[\"figure.figsize\"] = [50,20]\n",
    "    #fig.canvas.tooalbar_visible = False\n",
    "    fig.canvas.header_visible = False\n",
    "    #fig.canvas.footer_visible = False\n",
    "    c1 = ax1.contourf(x1_ar, x2_ar, f_ar, cmap=\"viridis\")\n",
    "    plt.colorbar(c1, ax = ax1)\n",
    "    ax1.set_title(r'$f(x)$')\n",
    "\n",
    "    c2 = ax2.contourf(x1_ar, x2_ar, grad_f_ar, cmap=\"viridis\")\n",
    "    plt.colorbar(c2, ax = ax2)\n",
    "    ax2.set_title(r'$\\||\\nabla f(x)\\||$')\n",
    "    #fig.set_size_inches(24, 10)\n",
    "\n",
    "    ws, fs, grad_fs = stochastic_gradient_descent(f, grad_f, stoch_grad_f, rng, np.array([w1_0, w2_0]), \\\n",
    "                                                       stepsize=constant_step_size, Nsteps=nsteps+1)\n",
    "\n",
    "    ax1.plot([0],[0],'*',ms = 20, color=\"orange\", label='solution', markeredgecolor=\"black\")\n",
    "    ax2.plot([0],[0],'*',ms = 20, color=\"orange\", label='solution', markeredgecolor=\"black\")\n",
    "\n",
    "\n",
    "    ax1.set_xlabel(r'$x_1$')\n",
    "    ax1.set_ylabel(r'$x_2$')\n",
    "    ax2.set_xlabel(r'$x_1$')\n",
    "    ax2.set_ylabel(r'$x_2$')\n",
    "\n",
    "    print(f'After {nsteps} iterarions GD stopped in [{round(ws[-1, 0], 3),round(ws[-1, 1], 3)}] with function value  f = {round(f(ws[-1, 0], ws[-1, 1]), 3)} ')\n",
    "\n",
    "    ax1.plot(ws[:,0], ws[:,1], 'ro-',label='GD')\n",
    "    ax2.plot(ws[:,0], ws[:,1], 'ro-', label='GD')\n",
    "\n",
    "    ax1.plot([w1_0],[w2_0],'^',ms = 20, color=\"white\", label=r'$x^0$',markeredgecolor=\"black\")\n",
    "    ax2.plot([w1_0],[w2_0],'^',ms = 20, color=\"white\", label=r'$x^0$',markeredgecolor=\"black\")\n",
    "\n",
    "    ax1.legend()\n",
    "    ax2.legend()\n",
    "    size = 20\n",
    "    plt.rcParams['xtick.labelsize'] = size  # 40\n",
    "    plt.rcParams['ytick.labelsize'] = size  # 40\n",
    "    plt.rcParams['legend.fontsize'] = size  # 30\n",
    "    plt.rcParams['axes.titlesize'] = size  # 40\n",
    "    plt.rcParams['axes.labelsize'] = size  # 40s\n",
    "\n",
    "    #fig.suptitle(f'Gradient with constant stepsize; {nsteps} steps; stepsize = {round(constant_step_size, 4)}')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    #FuncAnimation(f, update, frames=range(nsteps), interval=120,init_func=init, blit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Exercise I\n",
    "\n",
    "1. Change only the stepsize of the algorithm and describe what happens when increasing/decreasing its magnitude (by factors of 2, 5, 10, your choice?)\n",
    "2. Reset the stepsize to its initial value, and change now constants $a$ and $b$ -- look at cases when the ratio $\\frac{a}{b}$ equals 1, 10, 100, and 1000. What changes in the algorithm's behaviour and why?\n",
    "3. Reset $a$ and $b$ to initial values and change now values $c$ and $d$. What happens when decreasing both to nearly zero? What happends when increasing both to large values (say, around 10)? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "widgets.interactive(interactive_plot, nsteps=widgets.IntSlider(min=0, max=200, value=0, step=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now turn our attention to the problem of ridge logistic regression:\n",
    "$$\n",
    "\\min_{x \\in \\mathbb{R}^d} \\left\\{f(x) =  \\frac{1}{n} \\sum_{i=1}^n \\log\\left\\{1 + \\exp{\\left(-b_i\\langle a_i, x\\rangle\\right)}\\right\\} + \\frac{\\lambda}{2} \\|x\\|^2\\right\\},\n",
    "$$\n",
    "where $b_i \\in \\{-1, 1\\}$ signifies the label of the $i$-th datapoint, and $a_i \\in \\mathbb{R}^d$ denotes the feature vector of the $i$-th datapoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IhkM_btbXgr"
   },
   "source": [
    "## Dataset Generation\n",
    "\n",
    "The dataset is generated utilizing the [sklearn.datasets.make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) function. Execute the following two cells and take note of the clear separation achieved among the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ytJTU9Mu4Fv"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n = 100\n",
    "d = 5\n",
    "\n",
    "data_matrix, labels = make_classification(n_samples = n, n_features=d, n_redundant=0, n_informative=3, class_sep = 3.0, n_clusters_per_class=1)\n",
    "labels = 2*labels - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "r8uVIQ260GT4",
    "outputId": "6c1011b8-f6b5-4f13-c49d-596816ea992e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.scatter(data_matrix[:, 1], data_matrix[:, 2], marker=\"o\", c=labels, s=50, edgecolors=\"face\", cmap=\"bwr\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uvEkLmxbc8R"
   },
   "source": [
    "## Implementation of the Objective Function and its Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIsPIuSdP1-G"
   },
   "source": [
    "Let us revisit the objective function of this exercise:\n",
    "$$\n",
    "\\min_{x \\in \\mathbb{R}^d} \\left\\{f(x) = \\frac{1}{n}\\sum_{i=1}^n f_i(x) \\right\\},\n",
    "$$\n",
    "where each individual function $f_i$ is defined as follows:\n",
    "$$\n",
    "f_i(x) = \\log\\left\\{1 + \\exp{\\left(-b_i\\langle a_i, x\\rangle\\right)}\\right\\} + \\frac{\\lambda}{2} \\|x\\|^2.\n",
    "$$\n",
    "\n",
    "The gradient of $f_i$ can be computed via the formula:\n",
    "$$\n",
    "\\nabla f_i(x) = - \\frac{1}{1 + \\exp{\\left(b_i\\langle a_i, x\\rangle\\right)}} b_i a_i + \\lambda x.\n",
    "$$\n",
    "\n",
    "Now, let us apply our knowledge of NumPy. Implement the individual function using NumPy functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TdorCCzhNTr3"
   },
   "outputs": [],
   "source": [
    "def individual_log_loss(weights, features, label, regularizer):\n",
    "  \"\"\" Calculate the ridge logistic loss function \"\"\"\n",
    "  # Replace pass with your code\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YiN1MRtGSuJR"
   },
   "outputs": [],
   "source": [
    "def individual_log_loss_grad(weights, features, label, regularizer):\n",
    "  \"\"\" Calculate the ridge logistic loss gradient \"\"\"\n",
    "\n",
    "  # Replace pass with your code\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGlRBaLjUhbl"
   },
   "outputs": [],
   "source": [
    "# def log_loss_by_ind(weights, reg, index):\n",
    "#   \"\"\" Calculate the ridge logistic loss for the i-th datapoint \"\"\"\n",
    "#   return individual_log_loss(weights, data_matrix[i], labels[i], reg)\n",
    "\n",
    "# def log_loss_grad_by_ind(weights, reg, index):\n",
    "#   \"\"\" Calculate the ridge logistic loss gradient for the i-th datapoint  \"\"\"\n",
    "#   return individual_log_loss_grad(weights, data_matrix[i], labels[i], reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ZHwuCN-V5wj"
   },
   "outputs": [],
   "source": [
    "def log_loss(weights, data_matrix, labels, regularizer):\n",
    "  \"\"\" Calculate the ridge logistic loss for all datapoints \"\"\"\n",
    "\n",
    "  log_loss_vector = np.log(1. + np.exp(- labels[:, None] * (data_matrix @ weights)))\n",
    "  log_loss = log_loss_vector.mean()\n",
    "\n",
    "  return log_loss + regularizer * np.linalg.norm(weights, 2) ** 2 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zb-qaj9ZW7o4"
   },
   "outputs": [],
   "source": [
    "def log_loss_grad(weights, data_matrix, labels, regularizer):\n",
    "  \"\"\" Calculate the ridge logistic loss gradient \"\"\"\n",
    "\n",
    "  label_by_features = labels[:, None] * data_matrix\n",
    "\n",
    "  scalers = - 1. / (1 + np.exp(label_by_features @ weights))\n",
    "\n",
    "  first_part = (scalers[:, None] * label_by_features).mean(axis=0)\n",
    "  second_part = regularizer * weights\n",
    "\n",
    "  return first_part + second_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RyFhMf8_05jM",
    "outputId": "b7a263a4-0300-4b34-a013-2444e598bc18"
   },
   "outputs": [],
   "source": [
    "x_0 = np.ones(5)\n",
    "reg = 1.\n",
    "log_loss_grad(x_0, data_matrix, labels, reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1zKq9YDbnZO"
   },
   "source": [
    "## Obtaining a Solution using Gradient Descent\n",
    "\n",
    "We employ a standard Gradient Descent approach to obtain the minimizer of the objective function. This solution will be essential for the purpose of performance comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h6G_UGoS4FIV"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(data_matrix, labels, regularizer, start_iterate, stepsize, num_iterations):\n",
    "  \"\"\" Run Gradient Descent \"\"\"\n",
    "  x = np.copy(start_iterate)\n",
    "  loss_values = [log_loss(x, data_matrix, labels, regularizer)]\n",
    "\n",
    "  for _ in range(num_iterations):\n",
    "    # replace x = None with the gradient descent step \n",
    "    x = None\n",
    "    loss_values.append(log_loss(x, data_matrix, labels, regularizer))\n",
    "\n",
    "  return x, loss_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcLPFXkzb5aG"
   },
   "source": [
    "## Computing Important Constants\n",
    "\n",
    "Throughout this exercise, we will need to use smoothness constant of both the main objective function $f$ and individual objective functions $f_i$. We first note that the smoothness constant of $f_i$ can be defined as follows:\n",
    "$$\n",
    "L_i = \\frac{1}{4} \\|a_i\\|^2 + \\lambda\n",
    "$$\n",
    "(Optional) Exercise 1. Prove that this constant, indeed, is a smoothness constant for $f_i$.\n",
    "\n",
    "In the same manner, one can also show that smoothness constant for the objective function $f$ can be obtained as follows:\n",
    "$$\n",
    "L = \\left\\|\\frac{1}{4n} \\textbf{A}_i^\\top \\textbf{A}_i + \\lambda \\textbf{I}_d \\right\\|,\n",
    "$$\n",
    "where $\\textbf{A} = \\begin{bmatrix} a_1^\\top \\\\ \\dots \\\\ a_n^\\top \\end{bmatrix}$ is the feature matrix and $\\textbf{I}_d$ is an identity matrix of size $d$.\n",
    "\n",
    "(Optional) Exercise 2. Prove the formula provides a smoothness constant for $f$.\n",
    "\n",
    "In the next cells, we compute these constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ATseaiS6-vz"
   },
   "outputs": [],
   "source": [
    "def constants(data_matrix, regularizer):\n",
    "  \"\"\" Compute smoothness constants \"\"\"\n",
    "  individual_smoothness_constants = []\n",
    "  for i in range(data_matrix.shape[0]):\n",
    "    individual_smoothness_constant = 0.25 * np.linalg.norm(data_matrix[i], 2) ** 2 + regularizer\n",
    "    individual_smoothness_constants.append(individual_smoothness_constant)\n",
    "\n",
    "  n, d = data_matrix.shape\n",
    "  objective_function_smoothness_constant = np.linalg.norm(data_matrix.transpose() @ data_matrix / (4 * n) + regularizer * np.eye(d))\n",
    "\n",
    "  return np.array(individual_smoothness_constants), objective_function_smoothness_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JhPhZcoRcxNM"
   },
   "outputs": [],
   "source": [
    "L_is, L = constants(data_matrix, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "id": "awoymossqwD3",
    "outputId": "13831740-bbed-4cf2-8ce7-80449b278838"
   },
   "outputs": [],
   "source": [
    "plt.hist(L_is)\n",
    "plt.xlabel(r'Individual smoothness constant $L_i$')\n",
    "plt.title(r'Distribution of constants $L_i$ across different datapoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XqMCm9gFDsmm",
    "outputId": "a9c18f16-aea9-4973-d14e-940bf0978b87"
   },
   "outputs": [],
   "source": [
    "print('Smoothness constant of the main objective equals', round(L, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0N96Z65HP-x"
   },
   "source": [
    "Now we are ready to compute the minimizer of $f$ with Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WN9ARqDgD4Cb"
   },
   "outputs": [],
   "source": [
    "x_0 = np.ones(d)\n",
    "stepsize = 1. / L\n",
    "num_iterations = 100\n",
    "\n",
    "x_solution, loss_values = gradient_descent(data_matrix, labels, reg, x_0, stepsize, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "Nc-WVqkcEP5A",
    "outputId": "61340355-8b17-44a5-b3d0-f32be8c77ea1"
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_values)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(r'Iteration $t$')\n",
    "plt.ylabel(r'$f(x^t)$')\n",
    "plt.xlim(left=0, right=num_iterations)\n",
    "# plt.ylim(bottom=0)\n",
    "plt.title('Gradient Descent on Ridge Logistic Regression')\n",
    "plt.grid(which='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIzJqfvQGvGS"
   },
   "source": [
    "## Lab Exercise II: Stochastic Gradient Descent with Uniform Sampling launched with different stepsizes\n",
    "\n",
    "In the first exercise, we are going to verify that the stepsize of SGD affects the size of the neigbourhood. Let us start with implementating SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K-V6hcBrGufg"
   },
   "outputs": [],
   "source": [
    "def SGD(data_matrix, labels, regularizer, x_solution, x_0, num_iterations, stepsize, p):\n",
    "  \"\"\" Run Stochastic Gradient Descent for the sampling strategy 'p' \"\"\"\n",
    "  rng = np.random.default_rng()\n",
    "  indices = rng.choice(data_matrix.shape[0], num_iterations, p=p)\n",
    "\n",
    "  dist_values = [np.linalg.norm(x_0 - x_solution, 2) ** 2]\n",
    "  x = np.copy(x_0)\n",
    "\n",
    "  for t in range(num_iterations):\n",
    "    i = indices[t]\n",
    "    # replace x = None with the stochastic gradient descent step; use that i is chosen at random between 0, 1, ..., n\n",
    "    x = None\n",
    "    dist_values.append(np.linalg.norm(x - x_solution, 2) ** 2)\n",
    "\n",
    "  return dist_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iu0xBEqD_-ib"
   },
   "source": [
    "Stochastic Gradient Descent with Uniform Sampling can be understood as a specific scenario within the broader context of Stochastic Gradient Descent. In this scenario, each data point is sampled with an equal probability of $\\frac{1}{n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TnfNkmVwOJwc",
    "outputId": "e92e82ac-861a-44b2-8991-40f707dd04a9"
   },
   "outputs": [],
   "source": [
    "p_uniform = np.ones(n) / n\n",
    "p_uniform[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ons7EhAGuIr"
   },
   "outputs": [],
   "source": [
    "SGD_US = partial(SGD, p=p_uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xA40d31-9MJj",
    "outputId": "719264bc-aa58-4674-c1cd-87c1326a8ba7"
   },
   "outputs": [],
   "source": [
    "print('The largest stepsize for SGD-US allowed by theory is', round(.5 / np.max(L_is), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixrKehvnBwLm"
   },
   "source": [
    "In this experiment, we are going to run Stochastic Gradient Descent with Uniform Sampling with 3 different stepsizes: 0.0003, 0.003, 0.3. Let us revisit the main theorem of the lecture. It states that if $f$ is $\\mu$-convex, and each $f_i$ is convex and $L_i$-smooth, then SGD with stepsize $0 < \\gamma \\leq \\frac{1}{2A''}$, where $A'' = max_i \\frac{L_i}{n p_i}$, satisfies\n",
    "$$\n",
    "\\mathbb{E} \\|x^k - x^\\ast \\|^2 \\leq (1 - \\gamma \\mu)^k\\|x^0 - x^\\ast\\|^2 + \\frac{2 \\gamma \\sigma_\\ast^2}{\\mu},\n",
    "$$\n",
    "where $\\sigma_\\ast^2 = \\mathbb{E}\\|g(x^\\ast)\\|^2 - \\|\\mathbb{E} g(x^\\ast) \\|^2$.\n",
    "\n",
    "(Optional) Exercise 3. Verify that utilized functions in this lab exercise satisfy all the requirements of the theorem.\n",
    "\n",
    "One notable implication of the theorem is observation that, although larger stepsize leads to faster convergence (the first element in the right-hand side converges faster to zero), it converges to the larger neigbourhood (the second element in the right-hand side grows linearly with $\\gamma$). Let us verify this fact on practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-CTq0Zh10aF"
   },
   "outputs": [],
   "source": [
    "x_0 = np.ones(d)\n",
    "num_iterations = 30000\n",
    "\n",
    "stepsizes = [3e-4, 3e-3, 3e-2]\n",
    "\n",
    "distances_list = []\n",
    "for stepsize in stepsizes:\n",
    "  distances = SGD_US(data_matrix, labels, reg, x_solution, x_0, num_iterations, stepsize)\n",
    "  distances_list.append(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "9an-xi-02ChY",
    "outputId": "0fcbb3a4-b0f3-4be3-ca04-5eb8e7d7bf95"
   },
   "outputs": [],
   "source": [
    "for ind, stepsize in enumerate(stepsizes):\n",
    "  plt.plot(distances_list[ind], label=r'$\\gamma=$' + str(stepsize))\n",
    "\n",
    "# plt.plot(distances, label='small stepsize')\n",
    "# plt.plot(distances_2, label='large stepsize')\n",
    "plt.yscale('log')\n",
    "plt.xticks(ticks=np.arange(stop=num_iterations + 1, step=num_iterations / 10), labels=np.arange(stop=num_iterations // 1000 + 1, step=num_iterations // 10000))\n",
    "plt.xlabel(r'Iteration $t$, $10^3$')\n",
    "plt.ylabel(r'$\\|x^t - x^\\ast\\|^2$')\n",
    "plt.xlim(left=0, right=num_iterations)\n",
    "plt.grid()\n",
    "plt.title('SGD-US launched with different stepsizes')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deADN0n_BLLz"
   },
   "source": [
    "What does the plot show?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_oowDEzDIuzw"
   },
   "source": [
    "## Lab Exercise III: Uniform vs. Importance Sampling\n",
    "\n",
    "In this exercise, we are going to explore three different sampling strategies: uniform and importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NGquwOK0FYnR"
   },
   "outputs": [],
   "source": [
    "p_importance = L_is / L_is.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "390dlWtfFYUj"
   },
   "outputs": [],
   "source": [
    "def stepsize(L_is, p):\n",
    "  return 0.5 * L_is.size / np.max(L_is / p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xthrz8FxFYJl",
    "outputId": "ca7b93a0-ea3f-4621-99b3-7496a81caeb3"
   },
   "outputs": [],
   "source": [
    "stepsize_importance = stepsize(L_is, p_importance)\n",
    "stepsize_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3TRN18onO9PZ",
    "outputId": "d170d1b1-2e35-4f49-a78b-2679e75b295a"
   },
   "outputs": [],
   "source": [
    "stepsize_uniform = stepsize(L_is, p_uniform)\n",
    "stepsize_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIyrsVc7N0Qs"
   },
   "outputs": [],
   "source": [
    "num_iterations = 1000\n",
    "x_0 = np.ones(d)\n",
    "\n",
    "distances_uniform = SGD(data_matrix, labels, reg, x_solution, x_0, num_iterations, stepsize_uniform, p_uniform)\n",
    "distances_importance = SGD(data_matrix, labels, reg, x_solution, x_0, num_iterations, stepsize_importance, p_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "HgqXA6QaNz2u",
    "outputId": "d3f09a21-8ab4-459a-ea41-934b14c80718"
   },
   "outputs": [],
   "source": [
    "plt.plot(distances_uniform, label='uniform')\n",
    "plt.plot(distances_importance, label='importance')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.xlim(left=0, right=num_iterations)\n",
    "plt.xlabel('Iteration t')\n",
    "plt.ylabel(r'$\\|x^t - x^\\ast\\|^2$')\n",
    "plt.title('SGD-US vs. SGD-IS')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLgoXfLoU4L0"
   },
   "source": [
    "How would you interpret the plot? Why?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
